{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import modeling\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime\n",
    "from sklearn import metrics\n",
    "logger = tf.get_logger()\n",
    "logger.propagate = False\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_hub = \"https://tfhub.dev/google/small_bert/bert_uncased_L-4_H-512_A-8/1\"\n",
    "model_output_dir = \"finetuned_weights/bert_small\"\n",
    "tf.gfile.MakeDirs(model_output_dir)\n",
    "num_labels = 3 \n",
    "hidden_size = 512\n",
    "is_predicting = True\n",
    "input_fn = bert.run_classifier.file_based_input_fn_builder(\"test\", 128, is_training=False, drop_remainder=False)\n",
    "config = modeling.BertConfig(vocab_size=30522, hidden_size=512, num_hidden_layers=4,\n",
    "                             num_attention_heads=8, intermediate_size=2048, type_vocab_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(features):\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    label_ids = features[\"label_ids\"]\n",
    "\n",
    "    with tf.variable_scope(\"module\"):\n",
    "        model = modeling.BertModel(config=config,\n",
    "                                   is_training=True,\n",
    "                                   input_ids=input_ids,\n",
    "                                   input_mask=input_mask,\n",
    "                                   token_type_ids=segment_ids,\n",
    "                                   use_one_hot_embeddings=False)\n",
    "\n",
    "    # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "    output_layer = model.get_pooled_output()\n",
    "\n",
    "    with tf.variable_scope(\"\", reuse=tf.AUTO_REUSE):\n",
    "        A = tf.get_variable(\"output_weights\", [hidden_size, num_labels], initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "        bias = tf.get_variable(\"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "\n",
    "    output_layer = tf.keras.layers.Dropout(rate=0.1)(output_layer, training= not is_predicting)\n",
    "    logits = tf.nn.xw_plus_b(output_layer, A, bias)\n",
    "\n",
    "    predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "    probs = tf.nn.softmax(logits,axis=-1)\n",
    "\n",
    "    xs = tf.get_default_graph().get_tensor_by_name(\"module/bert/embeddings/embedding_lookup/Identity:0\")\n",
    "    grad_m = tf.gradients(probs[:, 0], xs=[xs])[0]\n",
    "    grad_m = tf.reduce_sum(grad_m, axis=[0, 2, 3])\n",
    "    grad_f = tf.gradients(probs[:, 1], xs=[xs])[0]\n",
    "    grad_f = tf.reduce_sum(grad_f, axis=[0, 2, 3])\n",
    "    grad_n = tf.gradients(probs[:, 2], xs=[xs])[0]\n",
    "    grad_n = tf.reduce_sum(grad_n, axis=[0, 2, 3])\n",
    "\n",
    "\n",
    "    return {\"logits\": logits, \"predictions\" : predictions, \"probs\" : probs, \"input_ids\": input_ids,\n",
    "            \"grad_m\" : grad_m, \"grad_f\" : grad_f, \"grad_n\": grad_n, \"labels\": label_ids}\n",
    "\n",
    "dataset = input_fn({\"batch_size\": 1, \"drop_remainder\": False})\n",
    "iterator = dataset.make_one_shot_iterator()\n",
    "model_output = forward(iterator.get_next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(filename='test_checkpoint')\n",
    "tokenizer = bert.tokenization.FullTokenizer(vocab_file=\"vocab.txt\", do_lower_case=True)\n",
    "    \n",
    "results = []\n",
    "with tf.Session() as sess:\n",
    "    sess.run(initializer)\n",
    "    saver.restore(sess, 'finetuned_weights/bert_small/model.ckpt-312')\n",
    "    while True:\n",
    "        try:\n",
    "            result = sess.run(model_output)        \n",
    "            results.append(result)\n",
    "        except:\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results_raw.txt\", \"w+\") as f:\n",
    "    for i, result in enumerate(results):\n",
    "        for k, v in result.items():\n",
    "            result[k] = v.flatten()\n",
    "        label = result[\"labels\"]\n",
    "        prediciton = result[\"predictions\"]\n",
    "\n",
    "        print(\"Example: \", i, \"(correct)\" if label == prediciton else \"(wrong)\", \"\\n\", file=f)\n",
    "        words = tokenizer.convert_ids_to_tokens(result[\"input_ids\"])\n",
    "        text = \" \".join(words)\n",
    "\n",
    "        print(\"Label: \", label, \" prediction: \", prediciton, \" probs: \", result[\"probs\"], file=f)\n",
    "        print(file=f)\n",
    "        print(text, file=f)\n",
    "        print(file=f)\n",
    "\n",
    "        # Extract which words have the highest gradients\n",
    "        for name in [\"masculine\", \"feminine\", \"neutral\"]:\n",
    "            word_grads = defaultdict(int)\n",
    "            position_grads = result[\"grad_\"+name[0]]  # grad_m, grad_f, grad_n\n",
    "            for w, g in zip(words, position_grads):\n",
    "                word_grads[w] += g\n",
    "\n",
    "            highest_influence_on_output = sorted(word_grads.keys(), key=lambda k: -word_grads[k]**2)\n",
    "            top5_words = highest_influence_on_output[:5]\n",
    "\n",
    "            print(\"Words with high {} gradient (word, grad): \".format(name), file=f)\n",
    "            for w in top5_words:\n",
    "                  print(w.ljust(15, \"_\"), \"{:6.4e}\".format(word_grads[w]), file=f)\n",
    "            print(\"   if negative: word is making the example less {}\".format(name), file=f)\n",
    "            print(\"   if positive: word is making the example more {}\".format(name), file=f)\n",
    "            print(file=f)\n",
    "\n",
    "        print(\"__\"*20, file=f)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
